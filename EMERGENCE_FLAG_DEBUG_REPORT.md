# Emergence Flag Divergence Investigation Report

**Date:** November 16, 2025
**Issue:** `enable_emergence=True` yields different results from `False` when no meta-agents emerge
**Status:** âœ… RESOLVED

---

## Executive Summary

When `ENABLE_EMERGENCE=True` but no meta-agents actually emerge (no consensus reached), the training dynamics should be **identical** to `ENABLE_EMERGENCE=False`. However, they diverged due to a subtle bug in overlap detection within the gradient adapter used by hierarchical training.

**Root Cause:** The `_GradientSystemAdapter` used a looser overlap check that incorrectly included "ghost neighbors" with zero actual spatial overlap.

**Impact:** These ghost neighbors diluted softmax coupling weights, changing gradient magnitudes and convergence behavior.

**Fix:** Made adapter's overlap detection match `MultiAgentSystem`'s two-step verification.

---

## Technical Deep Dive

### The Two Training Paths

#### Path 1: Standard Training (`enable_emergence=False`)
```python
system = MultiAgentSystem(agents, system_cfg)
trainer = Trainer(system, training_cfg)
history = trainer.train()
```

**Key Components:**
- `MultiAgentSystem` computes overlap masks ONCE in `__init__`
- Overlaps stored as continuous arrays: `overlap_masks[(i,j)] = chi_i * chi_j`
- `Trainer.step()` applies updates using `compute_natural_gradients(system)`

#### Path 2: Hierarchical Training (`enable_emergence=True`)
```python
multi_scale_system = MultiScaleSystem(manifold)
# ... add base agents ...
engine = HierarchicalEvolutionEngine(multi_scale_system, hier_config)

for step in range(N_STEPS):
    active_agents = multi_scale_system.get_all_active_agents()
    temp_system = _GradientSystemAdapter(active_agents, system_cfg)  # âš ï¸ Created fresh each step
    gradients = compute_natural_gradients(temp_system)
    engine.evolve_step(gradients)
```

**Key Components:**
- `MultiScaleSystem` wraps agents at multiple scales
- `_GradientSystemAdapter` created FRESH each step to wrap active agents
- `HierarchicalEvolutionEngine.evolve_step()` applies updates

### The Divergence Point

The adapter computes overlaps in its `__init__` method:

#### âŒ **BUGGY CODE** (before fix):
```python
# simulation_suite.py:820-821
max_overlap = np.max(chi_i) * np.max(chi_j)
self._overlaps[(i, j)] = (max_overlap > overlap_threshold)
```

#### âœ… **CORRECT CODE** (`MultiAgentSystem`):
```python
# agent/system.py:220-237
# Check 1: Upper bound
max_overlap = np.max(chi_i) * np.max(chi_j)
if max_overlap < threshold:
    continue

# Check 2: Actual overlap
chi_ij = chi_i * chi_j  # Element-wise product!
if np.max(chi_ij) < threshold:
    continue

overlap_masks[(i, j)] = chi_ij
```

### Why the Difference Matters

The product of maximums `max(chi_i) Ã— max(chi_j)` is an **upper bound** on the actual overlap `max(chi_i * chi_j)`, but they're **not equal** when masks are spatially disjoint!

**Counterexample:**
```python
# Agent i at position 0, agent j at position 1
chi_i = np.array([1.0, 0.0])
chi_j = np.array([0.0, 1.0])

# Adapter's check (WRONG):
max_overlap = np.max(chi_i) * np.max(chi_j) = 1.0 * 1.0 = 1.0
if 1.0 > 0.001:  # âœ“ Passes threshold â†’ neighbor included

# Correct check:
chi_ij = chi_i * chi_j = [0.0, 0.0]
if np.max(chi_ij) > 0.001:  # âœ— Fails threshold â†’ neighbor excluded
```

### Propagation of the Bug

Even though these "ghost neighbors" have `chi_ij = 0` (contributing zero energy), they still affect training:

1. **Softmax weight dilution:** `get_neighbors()` returns extra indices
2. **Softmax computation** includes them in denominator:
   ```python
   beta_ij(c) = exp[-KL_ij(c)/Îº] / Î£_k exp[-KL_ik(c)/Îº]
                                    â†‘ includes ghosts!
   ```
3. **Smaller beta weights** â†’ different gradient magnitudes
4. **Different convergence** even though energy contributions are zero

---

## The Fix

**File:** `simulation_suite.py:819-829`

```python
# CRITICAL: Match MultiAgentSystem's two-check overlap logic
# Check 1: Upper bound (product of maxes)
max_overlap = np.max(chi_i) * np.max(chi_j)
if max_overlap < overlap_threshold:
    self._overlaps[(i, j)] = False
    continue

# Check 2: Actual overlap (max of products)
chi_ij = chi_i * chi_j  # Element-wise product
has_overlap = np.max(chi_ij) >= overlap_threshold
self._overlaps[(i, j)] = has_overlap
```

This ensures:
- âœ… Both paths use identical neighbor detection
- âœ… No ghost neighbors in softmax computation
- âœ… Identical gradients when no emergence occurs
- âœ… Identical convergence behavior

---

## Verification Plan

### Test 1: Gradient Comparison
```python
# Set identical initial conditions
np.random.seed(42)

# Path A: Standard training
system_std = build_standard_system()
grads_std = compute_natural_gradients(system_std)

# Path B: Hierarchical training (no emergence)
system_hier = build_hierarchical_system()
adapter = _GradientSystemAdapter(system_hier.agents[0], system_config)
grads_hier = compute_natural_gradients(adapter)

# Verify gradients match
for i in range(N_AGENTS):
    assert np.allclose(grads_std[i].delta_mu_q, grads_hier[i].delta_mu_q, rtol=1e-5)
    assert np.allclose(grads_std[i].delta_Sigma_q, grads_hier[i].delta_Sigma_q, rtol=1e-5)
    print(f"âœ“ Agent {i} gradients match")
```

### Test 2: Energy Comparison
```python
# Verify energy computations match
energy_std = compute_total_free_energy(system_std)
energy_hier = compute_total_free_energy(adapter)

assert np.isclose(energy_std.total, energy_hier.total, rtol=1e-6)
print(f"âœ“ Energies match: {energy_std.total:.6f}")
```

### Test 3: Full Training Convergence
```python
# Run both paths for N_STEPS with identical initialization
ENABLE_EMERGENCE = False
history_std = run_training(system_std, N_STEPS=50)

ENABLE_EMERGENCE = True
ENABLE_CROSS_SCALE_PRIORS = False  # Disable emergence features
history_hier = run_hierarchical_training(system_hier, N_STEPS=50)

# Verify final energies match
assert np.isclose(
    history_std['total_energy'][-1],
    history_hier['total_energy'][-1],
    rtol=1e-4
)
print("âœ“ Final energies converge to same value")
```

---

## Related Code Locations

### Overlap Detection
- **Standard:** `agent/system.py:142-242` (`_compute_overlap_masks`)
- **Adapter:** `simulation_suite.py:785-829` (`_GradientSystemAdapter.__init__`)

### Gradient Computation
- **Engine:** `gradients/gradient_engine.py:445-598` (`compute_belief_alignment_gradients`)
- **Softmax:** `gradients/softmax_grads.py:54-165` (`compute_softmax_weights`)

### Energy Computation
- **Main:** `free_energy_clean.py:413-469` (`compute_total_free_energy`)
- **Alignment:** `free_energy_clean.py:111-210` (`compute_belief_alignment_energy`)

### Training Loops
- **Standard:** `agent/trainer.py:147-188` (`Trainer.step`)
- **Hierarchical:** `meta/hierarchical_evolution.py:94-209` (`HierarchicalEvolutionEngine.evolve_step`)
- **Simulation:** `simulation_suite.py:714-986` (`run_hierarchical_training`)

---

## Lessons Learned

1. **Spatial reasoning subtlety:** `max(A * B) â‰  max(A) * max(B)` for arrays
2. **Adapter consistency:** Wrappers must EXACTLY match the interface they're replacing
3. **Two-stage validation:** Fast rejection + accurate verification prevents edge cases
4. **Softmax sensitivity:** Even zero-energy neighbors affect normalization
5. **Testing importance:** Need unit tests comparing standard vs hierarchical paths

---

## Recommendations

### Immediate
- âœ… **Fix applied** - Adapter now matches `MultiAgentSystem` overlap detection
- ðŸ”„ **Run verification tests** to confirm convergence

### Short-term
- Add unit test: `test_adapter_matches_system_overlaps()`
- Add integration test: `test_emergence_disabled_matches_standard()`
- Document adapter's exact interface requirements

### Long-term
- Consider unifying overlap detection in shared utility
- Add runtime assertion comparing adapter vs system neighbor counts
- Create debugging mode that logs neighbor lists at each step

---

## Conclusion

The bug was subtle but critical: incorrect overlap detection created "ghost neighbors" that diluted softmax weights, causing different training dynamics even when hierarchical features were disabled. The fix ensures both training paths are mathematically equivalent when emergence is disabled, providing a solid foundation for testing hierarchical dynamics.

**Status:** Bug identified, fixed, and committed. Ready for verification testing.
